{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c52b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.gutenberg.org/ebooks/22566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb786e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "device = torch.device(\"mps\")\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "#eval_interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8524a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2167f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chars)\n",
    "# print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e55e71",
   "metadata": {},
   "source": [
    "#### generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d495f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping\n",
    "string_to_int = {ch:i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encoding-decoding tokenizer (character level)\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b3124",
   "metadata": {},
   "source": [
    "#### validation and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d099698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[54,  1, 55, 68, 78,  9,  1, 61],\n",
      "        [58, 66, 23,  1, 54, 67, 57,  1],\n",
      "        [67,  1, 27, 61, 62, 56, 54, 60],\n",
      "        [54,  1, 78, 58, 65, 65, 68, 76]], device='mps:0')\n",
      "targets:\n",
      "tensor([[ 1, 55, 68, 78,  9,  1, 61, 68],\n",
      "        [66, 23,  1, 54, 67, 57,  1, 67],\n",
      "        [ 1, 27, 61, 62, 56, 54, 60, 68],\n",
      "        [ 1, 78, 58, 65, 65, 68, 76,  1]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(x)\n",
    "print(\"targets:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa9a652",
   "metadata": {},
   "source": [
    "#### input-output implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_data[:block_size]\n",
    "# y = train_data[1:block_size+1]\n",
    "\n",
    "# for t in range(block_size):\n",
    "#     context = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print(f\"when input is {context} target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3f5bb",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a239124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # decorator that doesnt allow gradient calc\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # dropout is turned off at evaluation\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5090fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n8jm-19cf'PyZPfbi.WgJVH3b0nyPEZTCEdPS9V:r?EFKL'OFBKDKZ2hF!KUnS\"!T_\n",
      "7i*qE9'emw4u_T?s*pS-KI(c[W9[_\n",
      ";nu8AEMC7z? (!gm 6_bpm\n",
      "_Mfpocitvx7BQYnS*)9bg O-l\"uYrUmot;D0\n",
      "8]B*7W?Bhz'qMS'!G15pqPQ.s-27z3kO-CaX(UF\"W5E)9nENFKWY\".AH-heiD)l*y_THlJ5o(cz)9.SV83f_qL8L))-z&\"Wt;Rt.-v O*_:[E?k*Tm-_Al(prCa]K6H)AZkJS3psl!e:LO,]cqp-!mMwJxpe:9'h)\n",
      "y-x.4hC*-OiG83S\"-Y-27-m?Bp4pr2;c**!SafDnE*edm:jLQ]WjLo\n",
      ".0HLp8bwO?BYd Bl)mERY2)NP5SRnvEwycUHcQS-\n",
      "lPaS7FCi2*&f:uf(_GQ'S3aB4Fd?86;I2nF'XE*z?15r?Ik;YIhCZfQV89'X9Hc6&nHEEVn42nRh)i;.On8D\"\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sample index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) # torch.long ==> int64\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a697eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train loss 2.4316, val loss 2.4774\n",
      "step 250, train loss 2.4197, val loss 2.4976\n",
      "step 500, train loss 2.4572, val loss 2.4916\n",
      "step 750, train loss 2.4583, val loss 2.4954\n",
      "step 1000, train loss 2.4327, val loss 2.4704\n",
      "step 1250, train loss 2.4479, val loss 2.4718\n",
      "step 1500, train loss 2.4292, val loss 2.4639\n",
      "step 1750, train loss 2.4639, val loss 2.5078\n",
      "step 2000, train loss 2.4323, val loss 2.4911\n",
      "step 2250, train loss 2.4433, val loss 2.5059\n",
      "step 2500, train loss 2.4408, val loss 2.4721\n",
      "step 2750, train loss 2.4492, val loss 2.4792\n",
      "step 3000, train loss 2.4624, val loss 2.4582\n",
      "step 3250, train loss 2.4497, val loss 2.5081\n",
      "step 3500, train loss 2.4771, val loss 2.4997\n",
      "step 3750, train loss 2.4379, val loss 2.4878\n",
      "step 4000, train loss 2.4642, val loss 2.4795\n",
      "step 4250, train loss 2.4356, val loss 2.4912\n",
      "step 4500, train loss 2.4540, val loss 2.5182\n",
      "step 4750, train loss 2.4560, val loss 2.4934\n",
      "step 5000, train loss 2.4445, val loss 2.4901\n",
      "step 5250, train loss 2.4375, val loss 2.4931\n",
      "step 5500, train loss 2.4361, val loss 2.5002\n",
      "step 5750, train loss 2.4330, val loss 2.5099\n",
      "step 6000, train loss 2.4330, val loss 2.4906\n",
      "step 6250, train loss 2.4511, val loss 2.4946\n",
      "step 6500, train loss 2.4334, val loss 2.4843\n",
      "step 6750, train loss 2.4558, val loss 2.4937\n",
      "step 7000, train loss 2.4423, val loss 2.5045\n",
      "step 7250, train loss 2.4378, val loss 2.4777\n",
      "step 7500, train loss 2.4345, val loss 2.4850\n",
      "step 7750, train loss 2.4401, val loss 2.4690\n",
      "step 8000, train loss 2.4401, val loss 2.5132\n",
      "step 8250, train loss 2.4329, val loss 2.4911\n",
      "step 8500, train loss 2.4302, val loss 2.5002\n",
      "step 8750, train loss 2.4652, val loss 2.4853\n",
      "step 9000, train loss 2.4450, val loss 2.4871\n",
      "step 9250, train loss 2.4347, val loss 2.4802\n",
      "step 9500, train loss 2.4531, val loss 2.4909\n",
      "step 9750, train loss 2.4494, val loss 2.4954\n",
      "2.4307284355163574\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64580957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "siroof ser awed ca tofle f burtr\n",
      "\"Hom, mo th t woucchorentou toporme ndory sopopawh?\"Whe IGutous ss us hean be thend the Sly llashe pthissooreral y. s s,\"_Em tho fosivendn tththeden ad'st q; f stirce wofrewol.\n",
      "\n",
      "b-l se athind  Proofur ZTh thasusthit f RODowatar w, gr\n",
      "thand simarom.\n",
      "tithof be methe furer thenss \" hagey.\n",
      "\n",
      "\"\n",
      "asathen whe t a ALve an s theinche kisond.\n",
      "aly o at; \" talyond  he aspong.\"\n",
      "\n",
      "\"bier th tl sy is the ered, seve se Iftame erlod irer mbed andoucis wed as theecon w, thyonde icazo\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e509e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
